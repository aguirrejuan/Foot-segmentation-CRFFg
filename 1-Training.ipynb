{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FHprgMdXppEm"
      },
      "source": [
        "# Training "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The experiments are logged using Wandb "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDT5AUwtcIr0",
        "outputId": "db85085f-3d86-4ff4-e031-41e450f76f79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for convRFF (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for gdown (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for gcpds.image_segmentation (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for tf-keras-vis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -U git+https://github.com/aguirrejuan/ConvRFF.git --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvJTAf_Zbtcp",
        "outputId": "e91f9f36-edcc-4448-953e-f90c48e75d4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Create sweep with ID: hg11kida\n",
            "Sweep URL: https://wandb.ai/colab11/thesis_experiments/sweeps/hg11kida\n"
          ]
        }
      ],
      "source": [
        "import wandb \n",
        "\n",
        "sweep_config = {\n",
        "    'method':'grid'\n",
        "}\n",
        "\n",
        "parameters_dict = {\n",
        "    'model': {\n",
        "        'values':['f_b','r_b','u_b','f_b_s_m3','r_b_s_m3','u_b_s_m3', 'f_b_s', 'f_r_s', \n",
        "                   'f_r_s_m1','r_b_s','r_r_s','r_r_s_m1', 'u_r_s_m1','u_r_s','u_b_s','u_r_b']\n",
        "        },\n",
        "    'dataset':{\n",
        "        'values':['infrared_thermal_feet_nodataaug', 'infrared_thermal_feet']\n",
        "    }\n",
        "    }\n",
        "sweep_config['parameters'] = parameters_dict\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project='thesis_experiments')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmJlab_6-CZf",
        "outputId": "b5348960-f129-48ed-9d5b-c175a33399f5"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import regularizers\n",
        "\n",
        "\n",
        "from functools import partial \n",
        "from convRFF import train as train_model\n",
        "from gcpds.image_segmentation.datasets.segmentation import NerveUtp\n",
        "from gcpds.image_segmentation.datasets.segmentation import BrachialPlexus\n",
        "from gcpds.image_segmentation.datasets.segmentation import OxfordIiitPet\n",
        "from gcpds.image_segmentation.datasets.segmentation import InfraredThermalFeet\n",
        "import wandb\n",
        "\n",
        "from gcpds.image_segmentation.models import (fcn_baseline,\n",
        "                                             unet_baseline,\n",
        "                                             res_unet_baseline\n",
        ")\n",
        "from convRFF.models.unet import unet_b_skips, unet_rff_skips, unet_rff_backbone\n",
        "from convRFF.models.fcn import fcn_b_skips, fcn_rff_skips, fcn_rff_backbone\n",
        "from convRFF.models.res_unet import res_unet_b_skips, res_unet_rff_skips, res_unet_rff_backbone\n",
        "\n",
        "\n",
        "#===============================================================================\n",
        "import tensorflow as tf \n",
        "from gcpds.image_segmentation.losses import DiceCoefficient\n",
        "from gcpds.image_segmentation.metrics import (Jaccard, \n",
        "                                              Sensitivity,\n",
        "                                              Specificity,\n",
        "                                              DiceCoefficientMetric)\n",
        "\n",
        "\n",
        "\n",
        "def compile_parameters():\n",
        "  return {'loss':DiceCoefficient(),\n",
        "          'optimizer':tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "          'metrics':[Jaccard(),\n",
        "                     Jaccard(name='Jaccard_0',target_class=0),\n",
        "                     Jaccard(name='Jaccard_1',target_class=1),\n",
        "                     Jaccard(name='Jaccard_2',target_class=2),\n",
        "                     Sensitivity(),\n",
        "                     Sensitivity(name='Sensitivity_0',target_class=0),\n",
        "                     Sensitivity(name='Sensitivity_1',target_class=1),\n",
        "                     Sensitivity(name='Sensitivity_2',target_class=2),\n",
        "                     Specificity(),\n",
        "                     Specificity(name='Specificity_0',target_class=0),\n",
        "                     Specificity(name='Specificity_1',target_class=1),\n",
        "                     Specificity(name='Specificity_2',target_class=2),\n",
        "                     DiceCoefficientMetric(),\n",
        "                     DiceCoefficientMetric(name='DiceCoeficienteMetric_0',target_class=0),\n",
        "                     DiceCoefficientMetric(name='DiceCoeficienteMetric_1',target_class=1),\n",
        "                     DiceCoefficientMetric(name='DiceCoeficienteMetric_2',target_class=2),\n",
        "                     ]\n",
        "  }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_model(model, input_shape, out_channels, kernel_regularizer):\n",
        "    MODELS = {'u_b':unet_baseline(input_shape=input_shape, out_channels=out_channels),\n",
        "              'u_b_s':unet_b_skips(input_shape=input_shape,out_channels=out_channels,kernel_regularizer=kernel_regularizer),\n",
        "              'u_r_s':unet_rff_skips(input_shape=input_shape,out_channels=out_channels,kernel_regularizer=kernel_regularizer, mul_dim=3),\n",
        "              'u_r_s_m1':unet_rff_skips(input_shape=input_shape,out_channels=out_channels,kernel_regularizer=kernel_regularizer, mul_dim=1),\n",
        "              'u_b_s_m3':unet_b_skips(input_shape=input_shape,out_channels=out_channels,kernel_regularizer=kernel_regularizer, mul_dim=3) ,\n",
        "              'u_r_b':unet_rff_backbone(input_shape=input_shape,out_channels=out_channels,kernel_regularizer=kernel_regularizer, mul_dim=3),\n",
        "              'f_b':fcn_baseline(input_shape=input_shape,out_channels=out_channels),\n",
        "              'r_b':res_unet_baseline(input_shape=input_shape,out_channels=out_channels),\n",
        "              'f_b_s':fcn_b_skips(input_shape=input_shape,out_channels=out_channels,kernel_regularizer=kernel_regularizer) ,\n",
        "              'f_r_s':fcn_rff_skips(input_shape=input_shape,out_channels=out_channels,kernel_regularizer=kernel_regularizer) ,\n",
        "              'f_r_s_m1':fcn_rff_skips(input_shape=input_shape,out_channels=out_channels,kernel_regularizer=kernel_regularizer,mul_dim=1),\n",
        "              'f_b_s_m3':fcn_b_skips(input_shape=input_shape,out_channels=out_channels,kernel_regularizer=kernel_regularizer,mul_dim=3),\n",
        "              'r_b_s':res_unet_b_skips(input_shape=input_shape,out_channels=out_channels,kernel_regularizer=kernel_regularizer),\n",
        "              'r_b_s_m3':res_unet_b_skips(input_shape=input_shape,out_channels=out_channels,kernel_regularizer=kernel_regularizer,mul_dim=3),\n",
        "              'r_r_s':res_unet_rff_skips(input_shape=input_shape,out_channels=out_channels,kernel_regularizer=kernel_regularizer), \n",
        "              'r_r_s_m1':res_unet_rff_skips(input_shape=input_shape,out_channels=out_channels,kernel_regularizer=kernel_regularizer,mul_dim=1)\n",
        "              }\n",
        "    return MODELS[model]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from convRFF.train import get_compile_parameters as gcp\n",
        "from convRFF.train import get_train_parameters as gtp\n",
        "\n",
        "\n",
        "def get_t_p(*args,**kwargs):\n",
        "    f = gtp(*args,**kwargs)\n",
        "    f.update({'epochs':200})\n",
        "    return f\n",
        "\n",
        "def get_c_p(*args,**kwargs):\n",
        "    f = gcp(*args,**kwargs)\n",
        "    f.update({'optimizer':tf.keras.optimizers.Adam(learning_rate=1e-3)})\n",
        "    return f\n",
        "\n",
        "kwargs_data_augmentation = dict(repeat=7, \n",
        "                                   flip_left_right=True, \n",
        "                                   flip_up_down=False,\n",
        "                                   range_rotate=(-15,15),\n",
        "                                   translation_h_w=(0.10,0.10),\n",
        "                                   zoom_h_w=(0.15,0.15),\n",
        "                                   batch_size=16,\n",
        "                                   shape=224,\n",
        "                                    split=[0.1,0.1]\n",
        "                                   )\n",
        "kernel_regularizer =None\n",
        "\n",
        "get_compile_parameters = get_c_p\n",
        "get_train_parameters = get_t_p\n",
        "out_channels = 1\n",
        "input_shape = 224,224,1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(config=None):\n",
        "    with wandb.init(config=config) as run:\n",
        "        config= dict(run.config)\n",
        "        model = config['model']\n",
        "        name_dataset = config['dataset']\n",
        "\n",
        "        if name_dataset== 'infrared_thermal_feet': \n",
        "            data_augmentation = True\n",
        "        elif name_dataset== 'infrared_thermal_feet_nodataaug':\n",
        "            data_augmentation = False\n",
        "            kwargs_data_augmentation['repeat'] = 1\n",
        "\n",
        "        dataset_class = InfraredThermalFeet\n",
        "        model = get_model(model, input_shape, out_channels, kernel_regularizer)\n",
        "        train_model(model, dataset_class, run,  data_augmentation=data_augmentation, \n",
        "                                                get_compile_parameters=get_compile_parameters,\n",
        "                                                get_train_parameters=get_train_parameters,\n",
        "                                                **kwargs_data_augmentation)\n",
        "import os \n",
        "os.environ[\"WANDB_API_KEY\"] = \"YOUR_WANDB_API_KEY\"\n",
        "SWEEP_ID = 'YOUR_SWEEP_ID'\n",
        "wandb.agent(SWEEP_ID, train, count=1000, project='thesis_experiments')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
